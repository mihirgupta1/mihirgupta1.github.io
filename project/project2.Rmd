---
title: "COVID Vaccinations and Voting Trends in the 2020 Presidential Election in Texas"
author: "Mihir Gupta"
date: "5/10/2022"
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F, R.options=list(max.print=25,dplyr.print_max=25))
```

## 1. Introduction

Please note, I have imported the same datasets from project 1, and have copied the same wrangling and tidying from project 1 to use for project 2.

I have chosen four datasets. The first dataset, called casedata contains the total number of confirmed COVID-19 cases by county in the state of Texas from the beginning of the Covid-19 pandemic. I got this data from the [Texas Department of State Health Services](https://dshs.texas.gov/coronavirus/additionaldata/).

The second dataset, called vaccinedata contains various information regarding COVID-19 vaccinations by county including doses allocated to each county, doses administered, and breakdowns by age and priority status. The most important information that I will be using is the total amount of vaccinations administered, people with one dose, people who are fully vaccinated, and people with a booster shot. I also received this data from the [Texas Department of State Health Services](https://dshs.texas.gov/coronavirus/additionaldata/).

My third dataset, called electiondata, contains the breakdown by county of the results for the 2020 presidential election in Texas. It contains the total vote counts for each county and the breakdown by candidate. I got this data from [Wikipedia](https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_Texas).

My fourth dataset, called populationdata contains the breakdown of the population for each county along with other information for each county including the year it was established and the origin for the name of the county. I got this data from [Wikipedia](https://en.wikipedia.org/wiki/List_of_counties_in_Texas), which sourced its data from the [US Census Bureau](https://www.census.gov/quickfacts/fact/table/TX/PST045219).

These data are interesting to me because I think the politicization of the COVID-19 pandemic and vaccine is a very interesting topic that can help reveal associations between political affialiation and core values that influence vaccination choices.

I expect that counties that voted more heavily for Donald Trump would have lower vaccination rates and higher proportions of cases than counties that voted for Joe Biden in the 2020 presidential election.

```{r, echo=FALSE}
# Call the necessary packages
library(tidyverse)
library(psych)
library(factoextra)
library(cluster)
library(GGally)
library(ade4)
library(plotROC)
library(caret)
library(generics)
```

```{r}
#Import the data into RStudio
casedata <- read_csv("CaseCountData.csv")
casedata %>% head()
vaccinedata <- read_csv("COVID-19 Vaccine Data by County.csv")
vaccinedata %>% head()
electiondata <- read_csv("Election Data.csv")
electiondata %>% head()
populationdata <- read_csv("Population Data.csv")
populationdata %>% head()
```

### a. Tidying and Wrangling Before Joining

#### 1. Tidying electiondata

Before I can join all of these datasets, I need to tidy and wrangle them to ensure there are no issues in joining the data.

```{r}
#Begin with Tidying electiondata
# remove the % sign from eaach of the values so that the percent voted for each candidate can be used as a number
electiondata <- electiondata %>%
  separate(`Donald TrumpRepublican %`, into = c("Donald TrumpRepublican %", "DT%"), sep = "%") %>%
  separate(`Joe BidenDemocratic %`, into = c("Joe BidenDemocratic %", "JB%"), sep = "%") %>%
  separate(`Jo JorgensenLibertarian %`, into = c("Jo JorgensenLibertarian %", "JJ%"), sep = "%") %>%
  separate(`Howie HawkinsGreen %`, into = c("Howie HawkinsGreen %", "HH%"), sep = "%") %>%
  separate(`Other votes %`, into = c("Other votes %", "Ov%"), sep = "%") 

# Change the data type from character to numeric
class(electiondata$`Donald TrumpRepublican %`) <- "numeric"
class(electiondata$`Joe BidenDemocratic %`) <- "numeric"
class(electiondata$`Jo JorgensenLibertarian %`) <- "numeric"
class(electiondata$`Howie HawkinsGreen %`) <- "numeric"
class(electiondata$`Other votes %`) <- "numeric"
electiondata %>% head()

#Tidy populationdata
populationdata <- populationdata %>%
  separate("County", into = c("county", "c"), sep = " County")
populationdata%>% head()
```

First I removed the % from each value that had the percentage of the votes won by each candidate in each county. To do this, I separated the column by % to remove it. Then, I changed the datatype of the original column for each candidate containing the percentage of the vote that they won from character to numeric.

I also removed the word county from each of the county names in the populationdata dataset using the separate function.

#### 2. Wrangling

Now, I need to remove any extra columns that are not useful from each dataset

```{r}
#Remove Extra Columns from electiondata
electiondata <- electiondata %>%
  select(-`DT%`, -`JB%`, -`JJ%`, -`HH%`, -`Ov%`)
electiondata %>% head()

#Remove Extra columns from vaccinedata
vaccinedata <- vaccinedata %>%
  select(`County Name`, `Total Doses Allocated`, `Vaccine Doses Administered`, `People Vaccinated with at least One Dose`, `People Fully Vaccinated`, `People Vaccinated with Booster Dose`)
vaccinedata %>% head()

#Remove Extra Columns from populationdata
populationdata <- populationdata %>%
  select(county, Population)
populationdata %>% head()

```

To make the data much more useable before joining the datasets.

### b. Joining the Data

Now, I will get information from each dataset.

```{r}
# Code to get the total observations in each dataset
populationdata %>% dim()
electiondata %>% dim()
vaccinedata %>% dim()
casedata %>% dim()
```

The total number of observations in each dataset are as follows: The populationdata dataset has 254 unique IDs and 254 total observations. The electiondata dataset has 254 unique IDs and 254 total observations. The vaccinedata dataset has 258 unique IDs and 258 total observations. The casedata dataset has 256 unique IDs and 256 total observations.

All four datasets have all 254 county names in common (which I won't list here). The vaccinedata dataset has 4 extra IDs which are Other, Texas, Federal Long-Term Care Vaccination Program, and Federal Pharmacy Retail Vaccination Program. The casedata has 2 extra IDs which are Incomplete Address and Total.

Now I will join the data.

```{r}
#Join all of the data into a new dataset called countydata
countydata <- populationdata %>%
  inner_join(electiondata, by = c("county" = "County")) %>%
  inner_join(vaccinedata, by = c("county" = "County Name")) %>%
  inner_join(casedata, by = c("county" = "County"))
countydata %>% head()
```

To join all of the data into one dataset, countydata, I first used Inner Join on population data and election data, with the name of the county as the key. Using a similar method, I joined vaccinedata and then casedata. 4 observations were dropped from vaccinedata and 2 observations were dropped from casedata. Some issues that may arise is that any data that does not fit within the counties but still happened within the state of Texas, for example federal facilities, will not be included in the rest of the Data.

### c. Wrangling the Data 

I will now wrangle the data further.

```{r}
# Create a new variable Vaccination Rate which calculates the number of fully vaccinated people per person in each county
countydata <- countydata %>%
  mutate(`Vaccination Rate` = `People Fully Vaccinated`/`Population`)
countydata %>% head()

# Remove redundant variables by selecting for the main variables
countydata <- countydata %>%
  select(county, Population, `Donald TrumpRepublican %`, `Total Doses Allocated`, `Vaccine Doses Administered`, `Confirmed Cases`, `Vaccination Rate`)
```

I have added a new variable that gives the vaccination rate per county and removed certain redundant variables.

## 2. Exploratory Data Analysis

I will now create correlation matrix to determine the correlation coefficients for my variables.

``` {r}
# Build a correlation matrix between all numeric variables
countydata_num <- countydata %>%
  select_if(is.numeric)
cor(countydata_num, use = "pairwise.complete.obs")

# Make it pretty using a heatmap with geom_tile!
cor(countydata_num, use = "pairwise.complete.obs") %>%
  # Save as a data frame
  as.data.frame %>%
  # Convert row names to an explicit variable
  rownames_to_column %>%
  # Pivot so that all correlations appear in the same column
  pivot_longer(-1, names_to = "other_var", values_to = "correlation") %>%
  ggplot(aes(rowname, other_var, fill = correlation)) +
  # Heatmap with geom_tile
  geom_tile() +
  # Change the scale to make the middle appear neutral
  scale_fill_gradient2(low="red",mid="white",high="blue") +
  # Overlay values
  geom_text(aes(label = round(correlation,2)), color = "black", size = 4) +
  # Give title and labels
  labs(title = "Correlation matrix for the dataset countydata", x = "variable 1", y = "variable 2")

#Create a correlation matric with univariate and bicariate graphs
pairs.panels(countydata_num, 
             method = "pearson", # correlation coefficient method
             hist.col = "red", # color of histogram 
             smooth = FALSE, density = FALSE, ellipses = FALSE)

```

The Population and Total Doses Administered variables are very highly correlated, with a correlation coeffient of 1. However, the Vaccination Rate and the Percentage Vote Share for Donald Trump in the 2020 Presidential Election are also highly negatively coorelated, with a coorelation coefficient of -0.74, suggesting that counties that voted for Donald Trump are very unlikely to have a high vaccination rate. Some variables that were not as correlated were the Population and Vaccination Rate, as well as the Confirmed Cases and Vaccination Rate, suggesting that Vaccination Rate does not have that much of an effect on the Total Confirmed Cases (however, this can be explained by looking at Total Doses Administered and Confirmed Cases, which are highly correlated). 

Some overall trends are that Population is highly correlated with Total Doses Allocated, Total Doses Administered, and Confirmed Cases, suggesting that the more people there were, more covid cases arose, and so the government gave more vaccine doses to counties with higher populations. 

## 3. Clustering: Performing k-means Clustering

Now, I will perform the necessary steps to perform k-means Clustering

### a. Scale the Data

```{r}
#scale the data
countydata2 <- countydata %>%
  select_if(is.numeric) %>%
  scale
head(countydata2)
```

### b. Determine Optimal Number of Clusters via Silhouette

Next, I will pick the optimal number of clusters. 

```{r}
fviz_nbclust(countydata2, kmeans, method = "silhouette")

```

From this, it seems that the optimal number of clusters would be 2 clusters. 

### c. Perform k-means Clustering with 2 Clusters

Now, I will perform the k-means clustering with 2 clusters.

```{r}
kmeans_results <- countydata2 %>%
  kmeans(2)
kmeans_results
names(kmeans_results)
kmeans_results$tot.withinss

#Assign each county to a cluster
countydata_kmeans <- countydata %>%
  mutate(cluster = as.factor(kmeans_results$cluster))

head(countydata_kmeans)
```

### d. Visualize Clusters

Now, I will visualize the data by final cluster assignment.

```{r}
# Visualize Cluster Data
fviz_cluster(kmeans_results, data = countydata2)

# Interpreting the clusters using ggpairs
countydata_kmeans2 <- countydata_kmeans %>%
  select(-county)
ggpairs(countydata_kmeans2, columns = 1:2, aes(color = cluster))


```

### e. Discussion of k-means results

It appears that the counties with the highest populations are in one cluster and the remaining counties are in the second cluster. The cluster means for Population, Total Doses Allocated, Vaccine Doses Administered, Confirmed Cases, and Vaccination Rates are extremely high for the first cluster and very low for the second cluster. The Percentage of Votes for Donald Trump in the 2020 presidential election is flipped. It is very low for the first cluster and comparitively high for the second cluster. For the first cluster, the 57th datapoint, being Dallas County is in the center. For the second cluster, it is very large with 249 data points and therefore difficult to determine which datapoint represents the center. 

## 4. Dimention Reduction using PCA

Now I will perform PCA Dimention Reduction. First I will prepare the data by scaling it

```{r}
#Prepare the Data by scaling it
countydata9 <- countydata %>% 
  select_if(is.numeric) %>%
  scale %>%
  as.data.frame
head(countydata9)
```

## a. Perform PCA

Next, I will perform the PCA.

```{r}
# Perform PCA on countydata9 dataset
pca <- countydata9 %>%
  prcomp()
```

### b. Choose the number of principal components

```{r}
#Choose the number of principal compenents via a scree plot
get_eigenvalue(pca)
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 40))
```

Keeping the first 2 components will add up to about 96%.

### c. Visualize and Interpret PCA Results

First, I will Visualize the PCA Results

```{r}
# Visualize the contributions of the variables to the PCs in a correlation circle
fviz_pca_var(pca, col.var = "black", repel = TRUE)

# Visualize both variables and individuals in the same graph
fviz_pca_biplot(pca, repel = TRUE)

```

Now I will interpret the PCA results. 

```{r}
# Visualzie the contribution of the individual datasets to the PCs in a table
get_pca_ind(pca)$contrib %>% as.data.frame

#Visualize the top 10 contribution of the individuals to the PCs
fviz_contrib(pca, choice = "ind", axes = 1, top = 10)
fviz_contrib(pca, choice = "ind", axes = 2, top = 10)

# Use the new dimentions (PCs) to represent the individuals
fviz_pca_ind(pca,  repel = TRUE) 

#Who contributed the most to each component
get_pca_var(pca)$coord %>% as.data.frame

# Visualize the 5 top contributions of the variables to the PCs as a percentage
fviz_contrib(pca, choice = "var", axes = 1, top = 5) # on PC1
fviz_contrib(pca, choice = "var", axes = 2, top = 5) # on PC2

```

The total variation explained by the first two PCAs is 95%. A low score of contributions indicates that the data from that point did not contribute to the variation for that PCA, whereas a high score of contribution indicates that the individual point contributed immensely to the variation of that point. For example, 101 (Harris County) Contributed immensely to Dimention 1 and Dimention 2. 
Moreover, the individual variables contributed to dimention 1 almost equally (Except for the Percentage Vote Share for Donald Trump in 2020). For dimention 2, Vaccination rate and the percentage vote share for Donald Trump in 2020 contributed very highly while the other variables did not contribute as much.

## 5. Clustering and Cross-Validation

### a. Performing Clustering

I will perform Logistic Regression, with Vaccine Rate as the response variable, and % Vote Share for Donald Trump in the 2020 presidential election.

```{r}
#Make y values 0 or 1
countydata8 <- countydata %>%
  filter(`Vaccination Rate` <= 1) %>%
  mutate(vac = ifelse(`Vaccination Rate` >= 0.5, 1, 0)) #Make Vaccination Rate a dummy categorical variable

#Take a look at the regression line
ggplot(countydata8, aes(`Donald TrumpRepublican %`, vac)) + 
  geom_point() +
  geom_smooth(method = "glm", se = FALSE, 
              method.args = list(family = "binomial")) + 
  ylim(0,1)

#Build the logistic model
fit <- glm(vac ~ `Donald TrumpRepublican %`, data = countydata8, family = "binomial")
summary(fit)

```

Now I will use the model to calculate the predicted probability of vaccination rate for each county. 

```{r}
# Calculate a predicted probability
log_countydata8 <- countydata8  %>% 
  mutate(probability = predict(fit, type = "response"),
         predicted = ifelse(probability > 0.5, 1, 0)) %>%
  rownames_to_column("counties") %>% 
  select(county, `Donald TrumpRepublican %`, vac, probability, predicted)
head(log_countydata8)
```

Now I will compare the predicted value to the actual observation for vaccine rate. 

```{r}
# Visualize predicted and actual transmissions
ggplot(log_countydata8, aes(`Donald TrumpRepublican %`, vac)) +
  geom_point(aes(color = as.factor(predicted))) +
  geom_smooth(method = "glm", se = FALSE, 
              method.args = list(family = "binomial")) +
  ylim(0,1) + 
  geom_hline(yintercept = 0.5, lty = 2)
```

I will now take a look at the confusion matrix, ROC, and AUC

```{r}
# Confusion matrix: compare true to predicted condition
table(log_countydata8$vac, log_countydata8$predicted) %>% addmargins %>%
  head()

# ROC curve
ROC <- ggplot(log_countydata8) + 
  geom_roc(aes(d = vac, m = probability), n.cuts = 0)
ROC

# Calculate the area under the curve
calc_auc(ROC)
```

The area under the curve is 85.09 %

### b. Cross Validation

#### 1. Testing the Model

I will now perform k-fold Cross Validation.

First, I will test my model by splitting the data into a training set and a test set. 

```{r}
# Select a fraction of the data for training purposes
train <- sample_frac(countydata8, size = 0.5)

# Select the rest of the data for the test dataset
test <- anti_join(countydata8, train, by = "county")
```

Now I will fit the model onto the `train` set:

```{r}
# Fit a logistic model in the 
fit <- glm(vac ~ `Donald TrumpRepublican %`, data = train, family = "binomial")

# Results in a data frame for training data
df_train <- data.frame(
  probability = predict(fit, newdata = train, type = "response"),
  vac = train$vac,
  data_name = "training")

# Results in a data frame for test data
df_test <- data.frame(
  probability = predict(fit, newdata = test, type = "response"),
  vac = train$vac,
  data_name = "test")

# Combined results
df_combined <- rbind(df_train, df_test)
```

Now I will evaluate the performance of the classifier on the `train` and `test` sets:

```{r}
#Create ROC Curve
ROC <- ggplot(df_combined) + 
  geom_roc(aes(d = vac, m = probability, color = data_name, n.cuts = 0))
ROC

# Compare test and training AUCs
calc_auc(ROC)

```

#### 2. Perform k-fold cross-validation

I will now perform k-fold cross validation. First I will setup:

```{r}
# Choose number of folds
k = 10 

# Randomly order rows in the dataset
data <- countydata8[sample(nrow(countydata8)), ] 

# Create k folds from the dataset
folds <- cut(seq(1:nrow(data)), breaks = k, labels = FALSE)
```

Next, I will fit the model and repeat the process for each k-fold

```{r}
# Use a for loop to get diagnostics for each test set
diags_k <- NULL

for(i in 1:k){
  # Create training and test sets
  train <- data[folds != i, ] # all observations except in fold i
  test <- data[folds == i, ]  # observations in fold i
  
  # Train model on training set (all but fold i)
  fit <- glm(vac ~ `Donald TrumpRepublican %`, data = train, family = "binomial")
  
  # Test model on test set (fold i)
  df <- data.frame(
    probability = predict(fit, newdata = test, type = "response"),
    vac = test$vac)
  
  # Consider the ROC curve for the test dataset
  ROC <- ggplot(df) + 
    geom_roc(aes(d = vac, m = probability, n.cuts = 0))

  # Get diagnostics for fold i (AUC)
  diags_k[i] <- calc_auc(ROC)$AUC
}

# Average performance 
mean(diags_k)

```

My new classifier predicted new observations with approximately an 85% accuracy. I did not see any signs of over-fitting. 